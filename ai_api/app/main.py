"""
ë¡œì»¬ Ollama API ì„œë¹„ìŠ¤
- ë¡œì»¬ì—ì„œ ì‹¤í–‰ë˜ëŠ” Ollamaë¥¼ ì™¸ë¶€ì—ì„œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” API ì„œë¹„ìŠ¤
- ngrokì„ í†µí•´ í„°ë„ë§í•˜ì—¬ Render ë°±ì—”ë“œì—ì„œ ì ‘ê·¼ ê°€ëŠ¥
"""

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import httpx
import os
from typing import Optional
import uvicorn

app = FastAPI(
    title="Ollama Local API",
    description="ë¡œì»¬ Ollamaë¥¼ ì™¸ë¶€ì—ì„œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” API ì„œë¹„ìŠ¤",
    version="1.0.0"
)

class ChatRequest(BaseModel):
    message: str
    context: Optional[str] = ""
    model: Optional[str] = "llama2"

class ChatResponse(BaseModel):
    response: str
    model: str
    success: bool

class HealthResponse(BaseModel):
    status: str
    ollama_status: str
    model: str

@app.get("/", response_model=dict)
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "Ollama Local API Service",
        "version": "1.0.0",
        "endpoints": {
            "chat": "/api/chat",
            "health": "/api/health",
            "models": "/api/models"
        }
    }

@app.get("/api/health", response_model=HealthResponse)
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ - Ollama ì—°ê²° ìƒíƒœ í™•ì¸"""
    try:
        # Ollama ì„œë²„ ìƒíƒœ í™•ì¸
        async with httpx.AsyncClient() as client:
            response = await client.get("http://localhost:11434/api/tags", timeout=5.0)
            if response.status_code == 200:
                models = response.json()
                model_list = [model["name"] for model in models.get("models", [])]
                return HealthResponse(
                    status="healthy",
                    ollama_status="connected",
                    model=", ".join(model_list) if model_list else "no models"
                )
            else:
                return HealthResponse(
                    status="unhealthy",
                    ollama_status="disconnected",
                    model="unknown"
                )
    except Exception as e:
        return HealthResponse(
            status="unhealthy",
            ollama_status=f"error: {str(e)}",
            model="unknown"
        )

@app.get("/api/models")
async def get_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get("http://localhost:11434/api/tags", timeout=10.0)
            if response.status_code == 200:
                return response.json()
            else:
                raise HTTPException(status_code=500, detail="Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.post("/api/chat", response_model=ChatResponse)
async def chat_with_ollama(request: ChatRequest):
    """Ollamaì™€ ì±„íŒ…"""
    try:
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        if request.context:
            prompt = f"ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸: {request.context}\n\nì‚¬ìš©ì ì§ˆë¬¸: {request.message}\n\nìœ„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ì¹œê·¼í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."
        else:
            prompt = request.message

        # Ollama API í˜¸ì¶œ
        ollama_payload = {
            "model": request.model,
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "stream": False
        }

        async with httpx.AsyncClient() as client:
            response = await client.post(
                "http://localhost:11434/api/chat",
                json=ollama_payload,
                timeout=60.0
            )
            
            if response.status_code == 200:
                result = response.json()
                return ChatResponse(
                    response=result["message"]["content"],
                    model=request.model,
                    success=True
                )
            else:
                raise HTTPException(
                    status_code=500, 
                    detail=f"Ollama API ì˜¤ë¥˜: {response.status_code} - {response.text}"
                )
                
    except httpx.TimeoutException:
        raise HTTPException(status_code=504, detail="Ollama ì‘ë‹µ ì‹œê°„ ì´ˆê³¼")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì±„íŒ… ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

@app.post("/api/pull")
async def pull_model(model_name: str):
    """ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"http://localhost:11434/api/pull",
                json={"name": model_name},
                timeout=300.0  # ëª¨ë¸ ë‹¤ìš´ë¡œë“œëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ
            )
            
            if response.status_code == 200:
                return {"message": f"ëª¨ë¸ {model_name} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ", "success": True}
            else:
                raise HTTPException(
                    status_code=500, 
                    detail=f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {response.status_code} - {response.text}"
                )
                
    except httpx.TimeoutException:
        raise HTTPException(status_code=504, detail="ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œê°„ ì´ˆê³¼")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

if __name__ == "__main__":
    print("ğŸš€ Ollama Local API Service ì‹œì‘")
    print("ğŸ“¡ ë¡œì»¬ Ollama ì„œë²„: http://localhost:11434")
    print("ğŸŒ API ì„œë²„: http://localhost:8003")
    print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ì—”ë“œí¬ì¸íŠ¸:")
    print("   - GET  /api/health  : í—¬ìŠ¤ ì²´í¬")
    print("   - GET  /api/models  : ëª¨ë¸ ëª©ë¡")
    print("   - POST /api/chat    : ì±„íŒ…")
    print("   - POST /api/pull    : ëª¨ë¸ ë‹¤ìš´ë¡œë“œ")
    
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8003,
        reload=True,
        log_level="info"
    )
